{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "# Function to unpack the cifar-10 dataset.\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as file:\n",
    "        data = pickle.load(file, encoding='bytes')\n",
    "    return data[b'data'], data[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "#from utils import unpickle\n",
    "\n",
    "\n",
    "class LoadTrainingData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.trainX = []\n",
    "        self.trainY = []\n",
    "\n",
    "        data_dir = './cifar-10/training batches'\n",
    "        batches = os.listdir(data_dir)\n",
    "\n",
    "        for batch in batches:\n",
    "            batch_data, batch_labels = unpickle(os.path.join(data_dir, batch))\n",
    "            self.trainX.extend(batch_data)\n",
    "            self.trainY.extend(batch_labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.trainX[item], self.trainY[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_dir = './cifar-10/'\n",
    "b_size=128\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=b_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,padding=1,stride=1)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.bn1   = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3,padding=1,stride=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.bn1   = nn.Dropout(0.2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3,padding=1,stride=1)\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n",
    "        self.bn1   = nn.Dropout(0.2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3,padding=1,stride=1)\n",
    "        self.bn4   = nn.BatchNorm2d(128)\n",
    "        self.bn1   = nn.Dropout(0.2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3,padding=1,stride=1)\n",
    "        self.bn5   = nn.BatchNorm2d(256)\n",
    "        self.bn1   = nn.Dropout(0.2)\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size=3,padding=1,stride=1)\n",
    "        self.bn6   = nn.BatchNorm2d(512)\n",
    "        self.bn1   = nn.Dropout(0.2)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "\n",
    "        x = x.view(-1,512)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): Dropout(p=0.2, inplace=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls=classifier()\n",
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----[Epoch: 1, loss: 1.328186, Learning Rate: 0.003000]\n",
      "-----[Epoch: 2, loss: 0.698232, Learning Rate: 0.003000]\n",
      "-----[Epoch: 3, loss: 0.780278, Learning Rate: 0.003000]\n",
      "-----[Epoch: 4, loss: 0.795177, Learning Rate: 0.003000]\n",
      "-----[Epoch: 5, loss: 0.606134, Learning Rate: 0.003000]\n",
      "-----[Epoch: 6, loss: 0.636958, Learning Rate: 0.003000]\n",
      "-----[Epoch: 7, loss: 0.572791, Learning Rate: 0.003000]\n",
      "-----[Epoch: 8, loss: 0.508388, Learning Rate: 0.003000]\n",
      "-----[Epoch: 9, loss: 0.702625, Learning Rate: 0.003000]\n",
      "-----[Epoch: 10, loss: 0.511007, Learning Rate: 0.003000]\n",
      "-----[Epoch: 11, loss: 0.506918, Learning Rate: 0.003000]\n",
      "-----[Epoch: 12, loss: 0.567333, Learning Rate: 0.003000]\n",
      "-----[Epoch: 13, loss: 0.542285, Learning Rate: 0.003000]\n",
      "-----[Epoch: 14, loss: 0.601996, Learning Rate: 0.003000]\n",
      "-----[Epoch: 15, loss: 0.461050, Learning Rate: 0.003000]\n",
      "-----[Epoch: 16, loss: 0.480971, Learning Rate: 0.003000]\n",
      "-----[Epoch: 17, loss: 0.565889, Learning Rate: 0.003000]\n",
      "-----[Epoch: 18, loss: 0.466376, Learning Rate: 0.003000]\n",
      "-----[Epoch: 19, loss: 0.450632, Learning Rate: 0.003000]\n",
      "-----[Epoch: 20, loss: 0.507683, Learning Rate: 0.003000]\n",
      "-----[Epoch: 21, loss: 0.387551, Learning Rate: 0.003000]\n",
      "-----[Epoch: 22, loss: 0.475064, Learning Rate: 0.003000]\n",
      "-----[Epoch: 23, loss: 0.383988, Learning Rate: 0.003000]\n",
      "-----[Epoch: 24, loss: 0.443665, Learning Rate: 0.003000]\n",
      "-----[Epoch: 25, loss: 0.474023, Learning Rate: 0.003000]\n",
      "-----[Epoch: 26, loss: 0.533915, Learning Rate: 0.003000]\n",
      "-----[Epoch: 27, loss: 0.383882, Learning Rate: 0.003000]\n",
      "-----[Epoch: 28, loss: 0.338921, Learning Rate: 0.003000]\n",
      "-----[Epoch: 29, loss: 0.394863, Learning Rate: 0.003000]\n",
      "-----[Epoch: 30, loss: 0.394388, Learning Rate: 0.003000]\n",
      "-----[Epoch: 31, loss: 0.439425, Learning Rate: 0.003000]\n",
      "-----[Epoch: 32, loss: 0.326636, Learning Rate: 0.003000]\n",
      "-----[Epoch: 33, loss: 0.326700, Learning Rate: 0.003000]\n",
      "-----[Epoch: 34, loss: 0.452133, Learning Rate: 0.003000]\n",
      "-----[Epoch: 35, loss: 0.343558, Learning Rate: 0.003000]\n",
      "-----[Epoch: 36, loss: 0.231213, Learning Rate: 0.003000]\n",
      "-----[Epoch: 37, loss: 0.243719, Learning Rate: 0.003000]\n",
      "-----[Epoch: 38, loss: 0.445412, Learning Rate: 0.003000]\n",
      "-----[Epoch: 39, loss: 0.365138, Learning Rate: 0.003000]\n",
      "-----[Epoch: 40, loss: 0.301493, Learning Rate: 0.003000]\n",
      "-----[Epoch: 41, loss: 0.303166, Learning Rate: 0.003000]\n",
      "-----[Epoch: 42, loss: 0.385570, Learning Rate: 0.003000]\n",
      "-----[Epoch: 43, loss: 0.479569, Learning Rate: 0.003000]\n",
      "-----[Epoch: 44, loss: 0.223269, Learning Rate: 0.003000]\n",
      "-----[Epoch: 45, loss: 0.211271, Learning Rate: 0.003000]\n",
      "-----[Epoch: 46, loss: 0.418456, Learning Rate: 0.003000]\n",
      "-----[Epoch: 47, loss: 0.225444, Learning Rate: 0.003000]\n",
      "-----[Epoch: 48, loss: 0.385082, Learning Rate: 0.003000]\n",
      "-----[Epoch: 49, loss: 0.421593, Learning Rate: 0.003000]\n",
      "-----[Epoch: 50, loss: 0.366326, Learning Rate: 0.003000]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, data, epoch,lr):\n",
    "    # define the loss function and back propagation algorithm\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr ,amsgrad=True,weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,80], gamma=0.1)\n",
    "\n",
    "    for e in range(epoch):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr = param_group['lr']\n",
    "        for i, dataset in enumerate(data):\n",
    "            inputs, lbl = dataset\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, lbl = inputs.cuda(), lbl.cuda()\n",
    "\n",
    "            # set the gradient for each parameters zero\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('-----[Epoch: %d, loss: %f, Learning Rate: %f]' % (e+1, loss.item(),lr  ))\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    print ('Finished Training')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cls=classifier()\n",
    "    if torch.cuda.is_available():\n",
    "        cls.cuda()\n",
    "    lr = 0.003\n",
    "    epoch = 50\n",
    "\n",
    "    train_model(cls, trainloader, epoch,lr)\n",
    "\n",
    "    # save model\n",
    "    torch.save(cls.state_dict(), './trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:30<00:00, 328.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:    76 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def test():\n",
    "    trained_model = './trained_model.pth'\n",
    "    \n",
    "    cls=classifier()\n",
    "    cls.load_state_dict(torch.load(trained_model))\n",
    "    cls.cuda()\n",
    "    cls.eval()\n",
    "\n",
    "    classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "    # calculating the accuracy of our classifier;\n",
    "    print(\"Calculating accuracy...\")\n",
    "    correct = 0\n",
    "    total = len(testloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dataset in tqdm(testloader):\n",
    "            inputs, lbl = dataset\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, lbl = inputs.cuda(), lbl.cuda()\n",
    "            \n",
    "            out = cls(inputs)\n",
    "            _, predicted = torch.max(out, 1)\n",
    "\n",
    "            # calculate the total accuracy\n",
    "            correct += (predicted == lbl).sum().item()\n",
    "        print('Accuracy: %5d %%' % (correct / total * 100))\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
